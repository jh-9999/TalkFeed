from deepface import DeepFace
import cv2
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from scipy.ndimage import gaussian_filter1d

# font_path = "C:/Windows/Fonts/malgun.ttf"
# font_prop = fm.FontProperties(fname=font_path, size=12)
# plt.rcParams['font.family'] = font_prop.get_name()

### 1ï¸âƒ£ ê°ì • ë¶„ì„ (DeepFace)
def analyze_video_with_feedback(video_path, output_file="feedback.txt"):
    try:
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            print(f"Error: Unable to open video file at {video_path}")
            return

        fps = int(cap.get(cv2.CAP_PROP_FPS))
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps
        print(f"ğŸ“½ï¸ Video loaded: {video_path}")
        print(f"â³ Duration: {duration:.2f}s, FPS: {fps}, Total Frames: {frame_count}")

        results = []
        frame_index = 0

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            if frame_index % (fps * 10) == 0:
                try:
                    print(f"ğŸ­ Analyzing frame at {frame_index}, Time: {frame_index / fps:.2f}s")
                    analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)

                    if isinstance(analysis, list) and analysis:
                        dominant_emotion = analysis[0].get('dominant_emotion', 'Unknown')
                    elif isinstance(analysis, dict):
                        dominant_emotion = analysis.get('dominant_emotion', 'Unknown')
                    else:
                        dominant_emotion = 'Unknown'

                    timestamp = frame_index / fps
                    if dominant_emotion != 'Unknown':
                        results.append((timestamp, timestamp + 10, dominant_emotion))
                        print(f"âœ” {timestamp:.2f}s - {dominant_emotion}")

                except Exception as e:
                    print(f"âŒ Error analyzing frame at {frame_index}: {e}")

            frame_index += fps * 10
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)

        cap.release()

        if results:
            with open(output_file, "w") as f:
                for start_time, end_time, emotion in results:
                    f.write(f"{start_time:.2f}-{end_time:.2f}s: {emotion}\n")
            print(f"ğŸ“„ ê°ì • ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        else:
            print("âš  No emotions detected.")

    except Exception as e:
        print(f"ğŸš¨ Error: {e}")

### 2ï¸âƒ£ STT ë¶„ì„ (Whisper)
def load_stt_results(file_path="downloads/sample.mp3.txt"):
    stt_data = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f.readlines():
                parts = line.strip().split("] ")
                if len(parts) == 2:
                    time_info = parts[0][1:]
                    text = parts[1]

                    start_time = float(time_info.split(" ~ ")[0])
                    stt_data.append((start_time, text))
        return stt_data
    except Exception as e:
        print(f"âŒ STT íŒŒì¼ ì˜¤ë¥˜: {e}")
        return []

### 3ï¸âƒ£ ê°ì • ë¶„ì„ ë°ì´í„° ë¡œë“œ
def load_emotion_results(file_path="feedback.txt"):
    emotion_data = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f.readlines():
                parts = line.strip().split(": ")
                if len(parts) != 2:
                    continue

                time_range, emotion = parts
                try:
                    start_time, end_time = time_range.replace("s", "").split("-")
                    start_time, end_time = float(start_time), float(end_time)
                    emotion_data.append((start_time, end_time, emotion))
                except ValueError:
                    continue

        return emotion_data
    except Exception as e:
        print(f"âŒ ê°ì • ë¶„ì„ íŒŒì¼ ì˜¤ë¥˜: {e}")
        return []



### 4ï¸âƒ£ **ëª¨ë“  êµ¬ê°„ì„ ì¶œë ¥í•˜ê³ , ê°ì •ê³¼ ëŒ€ì‚¬ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸**
def compare_emotion_and_speech(stt_data, emotion_data):
    results = []
    
    for stt_time, stt_text in stt_data:
        closest_emotion = None
        for emotion_start, emotion_end, emotion in emotion_data:
            if emotion_start <= stt_time <= emotion_end:
                closest_emotion = emotion
                break

        if closest_emotion:
            # ê°ì •ê³¼ ëŒ€ì‚¬ì˜ ì˜ë¯¸ ë¶„ì„ (ê¸ì •/ë¶€ì • ê°ì • ë¹„êµ)
            speech_sentiment = "positive" if any(word in stt_text for word in ["ê¸°ì˜ë‹¤", "ì¢‹ë‹¤", "í–‰ë³µ"]) else "negative"
            emotion_sentiment = "positive" if closest_emotion in ["happy", "surprise"] else "negative"
            
            match_status = "âœ… ì¼ì¹˜" if speech_sentiment == emotion_sentiment else "âŒ ë¶ˆì¼ì¹˜"
            results.append((stt_time, stt_text, closest_emotion, match_status))

    return results

def visualize_emotion_speech_comparison(comparison_results):
    timestamps = [result[0] for result in comparison_results]
    emotions = [result[2] for result in comparison_results]
    texts = [result[1] for result in comparison_results]
    match_status = [result[3] for result in comparison_results]

    emotion_mapping = {"happy": 3, "surprise": 2, "neutral": 1, "sad": -1, "angry": -2, "fear": -3, "disgust": -3}
    emotion_scores = [emotion_mapping.get(emotion, 0) for emotion in emotions]

    smoothed_scores = gaussian_filter1d(emotion_scores, sigma=2)

    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                        row_heights=[0.7, 0.3])

    # ì¼ì¹˜í•˜ëŠ” ì ê³¼ ë¶ˆì¼ì¹˜í•˜ëŠ” ì ì„ ë¶„ë¦¬
    match_times = [t for t, s in zip(timestamps, match_status) if s == "âœ… ì¼ì¹˜"]
    match_scores = [s for s, status in zip(emotion_scores, match_status) if status == "âœ… ì¼ì¹˜"]
    mismatch_times = [t for t, s in zip(timestamps, match_status) if s == "âŒ ë¶ˆì¼ì¹˜"]
    mismatch_scores = [s for s, status in zip(emotion_scores, match_status) if status == "âŒ ë¶ˆì¼ì¹˜"]

    # ì¼ì¹˜í•˜ëŠ” ì 
    fig.add_trace(
        go.Scatter(x=match_times, y=match_scores, mode='markers',
                   marker=dict(size=10, color='green', symbol='circle'),
                   name='ì¼ì¹˜'),
        row=1, col=1
    )

    # ë¶ˆì¼ì¹˜í•˜ëŠ” ì 
    fig.add_trace(
        go.Scatter(x=mismatch_times, y=mismatch_scores, mode='markers',
                   marker=dict(size=10, color='red', symbol='x'),
                   name='ë¶ˆì¼ì¹˜'),
        row=1, col=1
    )

    # ìŠ¤ë¬´ë”©ëœ ê°ì • ë³€í™” ì¶”ì„¸ì„ 
    fig.add_trace(
        go.Scatter(x=timestamps, y=smoothed_scores, mode='lines',
                   line=dict(color='blue', width=2),
                   name='ê°ì • ë³€í™” ì¶”ì„¸'),
        row=1, col=1
    )

    # ëŒ€ì‚¬ í‘œì‹œ (ìƒ˜í”Œë§)
    sample_rate = max(1, len(texts) // 20)
    sampled_texts = texts[::sample_rate]
    sampled_timestamps = timestamps[::sample_rate]

    fig.add_trace(
        go.Scatter(x=sampled_timestamps, y=[0] * len(sampled_timestamps), mode='markers+text',
                   marker=dict(symbol='circle', size=8, color='blue'),
                   text=sampled_texts,
                   textposition='top center',
                   name='ëŒ€ì‚¬'),
        row=2, col=1
    )

    emotion_labels = {3: "í–‰ë³µ", 2: "ë†€ëŒ", 1: "ì¤‘ë¦½", -1: "ìŠ¬í””", -2: "ë¶„ë…¸", -3: "ê³µí¬/í˜ì˜¤"}
    fig.update_layout(
        title="ê°ì •ê³¼ ë°œí™” ë‚´ìš© ì¼ì¹˜ë„",
        height=800,
        showlegend=True,
        hovermode="closest"
    )

    fig.update_yaxes(title_text="ê°ì • ì ìˆ˜", row=1, col=1,
                     ticktext=[emotion_labels[score] for score in sorted(emotion_labels.keys())],
                     tickvals=sorted(emotion_labels.keys()))
    fig.update_yaxes(title_text="ëŒ€ì‚¬", showticklabels=False, row=2, col=1)
    fig.update_xaxes(title_text="ì‹œê°„ (ì´ˆ)", row=2, col=1,
                     tickmode='array',
                     tickvals=np.arange(0, max(timestamps) + 60, 60),
                     ticktext=[f"{int(t/60)}:{int(t%60):02d}" for t in np.arange(0, max(timestamps) + 60, 60)])

    fig.show()

if __name__ == "__main__":
    try:
        video_path = "downloads/sample_video.mp4"
        
        analyze_video_with_feedback(video_path)
        stt_data = load_stt_results()
        emotion_data = load_emotion_results()

        if stt_data and emotion_data:
            comparison_results = compare_emotion_and_speech(stt_data, emotion_data)
            visualize_emotion_speech_comparison(comparison_results)
        else:
            print("ğŸš¨ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: ê°ì •-ëŒ€ì‚¬ ë¹„êµ ë¶ˆê°€")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()